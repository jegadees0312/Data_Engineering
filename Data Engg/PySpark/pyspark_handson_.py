# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9i1lEl9VEiclKKuz6bb5NmdrRYMy37X
"""

pip install pyspark

# Import PySpark and initialize a Spark session
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PySpark Notebook Example") \
    .getOrCreate()

# Verify the Spark session is working
spark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder \
       .appName('Pyspark Dataframe Example') \
       .getOrCreate()

# sample data
data = [
    ("John Doe","Engineering",75000),
    ("Jane Smith","Marketing",60000),
    ("Sam Brown","Marketing",80000),
    ("Emily Davis","HR",50000),
    ("Michael Johnson","Marketing",70000)
]

# schema defining
columns = ["Name","Department","Salary"]

# Creating Dataframe
df = spark.createDataFrame(data,schema=columns)

# showing the DataFrame
df.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder \
       .appName('Pyspark Dataframe Example') \
       .getOrCreate()

# sample data
data = [
    ("John Doe","Engineering",75000),
    ("Jane Smith","Marketing",60000),
    ("Sam Brown","Marketing",80000),
    ("Emily Davis","HR",50000),
    ("Michael Johnson","Marketing",70000)
]

# schema defining
columns = ["Name","Department","Salary"]

# Creating Dataframe
df = spark.createDataFrame(data,schema=columns)

# showing the DataFrame
df.show()


high_salary_df = df.filter(col("Salary") > 65000)
print("High salary above 65000")
high_salary_df.show()

pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize spark session
spark = SparkSession.builder \
       .appName('Customer Transaction Analysis') \
       .getOrCreate()

# sample data for customers
customes = [
    (1,"Ravi","Mumbai"),
    (2,"Priya","Delhi"),
    (3,"Vijay","Bangalore"),
    (4,"Anita","Chennai"),
    (5,"Raj","Hyderabad"),
]

# sample data for transactions
trasctions = [
    (1,1,10000.50),
    (2,2,20000.75),
    (3,1,15000.25),
    (4,3,30000.00),
    (5,2,40000.50),
    (6,4,25000.00),
    (7,5,18000.75),
    (8,1,5000.00),
]

# Define schema for DataFrames
customer_columns = ["customerID","Name","City"]
transaction_columns = ["TransactionID","CustomerID","Amount"]

# Create Dataframes
customer_df = spark.createDataFrame(customes,schema=customer_columns)
transaction_df = spark.createDataFrame(trasctions,schema=transaction_columns)

# Show tha DataFrames
print("Customer DataFrame:")
customer_df.show()

print("Transaction DataFrame:")
transaction_df.show()

# Join the DataFrame on CustomerId
customer_transactions_df = customer_df.join(transaction_df,on = "CustomerID")

# Show the joined DataFrame
print("Customer Transactions DataFrame:")
customer_transactions_df.show()

# Total amount spent by each customer
total_spent_df = customer_transactions_df.groupBy("Name").sum("Amount").withColumnRenamed("sum(Amount)","TotalSpent")

# Show the total amount spent by each customer
print("Total Amount Spent by Each Customer:")
total_spent_df.show()

# Find customers who have spent more than 35000
big_spent_df = total_spent_df.filter(col("TotalSpent") > 35000)

# Show the customers who spent more than 35000
print("Customers who spent more than 35000:")
big_spent_df.show()

# Count the Number of Transactions per customer
transaction_count_df = customer_transactions_df.groupBy("Name").count().withColumnRenamed("count","TransactionCount")

# Show the number of transactions per customer
print("Number of Transactions per Customer:")
transaction_count_df.show()

# Sort customers by total amount spent in descending order
sorted_spent_df = total_spent_df.orderBy(col("TotalSpent").desc())

# Show the customers sorted by total amount spent
print("Customers Sorted by Total Amount Spent:")
sorted_spent_df.show()