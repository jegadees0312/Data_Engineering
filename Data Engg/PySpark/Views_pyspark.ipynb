{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81PIyDyFPhsF",
        "outputId": "302cfb1d-c6c0-439b-8461-fdbdac0c1206"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creatinga spark session\n",
        "spark = SparkSession.builder \\\n",
        "     .appName('Data ingestion') \\\n",
        "     .getOrCreate()"
      ],
      "metadata": {
        "id": "w_nISJ8zYFTL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **READING CSV FILE USING PYSPARK**"
      ],
      "metadata": {
        "id": "oHr7d5-SZ8lz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ff_upSjcStn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"/content/sample_data/people.txt\"\n",
        "\n",
        "# Reading the csv file with pyspark\n",
        "df_csv =  spark.read.format(\"csv\").option(\"header\",\"true\").load(csv_file_path)\n",
        "df_csv.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "craszbjpYn5w",
        "outputId": "35bb2d1b-a16d-428a-c893-5b2cdd2625da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|Name| Age| Gender|\n",
            "+----+----+-------+\n",
            "|John|  28|   Male|\n",
            "|Jane|  32| Female|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **READING JSON FILE USING PYSPARK**"
      ],
      "metadata": {
        "id": "YuA8stJ2hd5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define the schema for the JSON file\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"street\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Loading the complex JSON file with schema\n",
        "json_file_path = \"/content/sample_data/JSON.txt\"\n",
        "\n",
        "# Read the json file with schema\n",
        "df_json_complex = spark.read.schema(schema).json(json_file_path)\n",
        "\n",
        "# Read the file as tesx to inspect its contents\n",
        "with open(json_file_path,\"r\") as f:\n",
        "  data = f.read()\n",
        "  print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LLbYXkXcZLi",
        "outputId": "bfa69185-0d75-494b-be63-9dfe88c29889"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"name\": \"John\",\n",
            "    \"age\": 28,\n",
            "    \"gender\": \"Male\",\n",
            "    \"address\": {\n",
            "      \"street\": \"123 Main St\",\n",
            "      \"city\": \"New York\"\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"name\": \"Jane\",\n",
            "    \"age\": 32,\n",
            "    \"gender\": \"Female\",\n",
            "    \"address\": {\n",
            "      \"street\": \"456 Elm St\",\n",
            "      \"city\": \"San Francisco\"\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VIEW**"
      ],
      "metadata": {
        "id": "eLPKL9Lslrk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample dataframe\n",
        "data = {\n",
        "    \"name\":[\"John\",\"Jane\",\"Mike\",\"Emily\"],\n",
        "    \"age\":[28,32,45,23],\n",
        "    \"gender\":[\"Mlae\",\"Female\",\"Male\",\"Female\"],\n",
        "    \"city\":[\"New York\",\"San Fransico\",\"Los Angeles\",\"Chicago\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# save the dataframe to a csv file in a colab environment\n",
        "csv_file_path = \"/content/sample_data/sample_people_data.csv\"\n",
        "df.to_csv(csv_file_path,index=False)\n",
        "\n",
        "# confirm the file is create\n",
        "print(f\"CSV file created at {csv_file_path}\")\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# initialize sparksession\n",
        "spark = SparkSession.builder.appName(\"Create View Example\").getOrCreate()\n",
        "\n",
        "# Load the csv file into a pyspark environment\n",
        "df_people = spark.read.format(\"csv\").option(\"header\",\"true\").load(csv_file_path)\n",
        "\n",
        "# show the DataFrame\n",
        "df_people.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGnyq0NYl1sg",
        "outputId": "676c8587-4f0d-41b8-9320-ff1c5720f31b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created /content/sample_data/sample_people_data.csv\n",
            "+-----+---+------+------------+\n",
            "| name|age|gender|        city|\n",
            "+-----+---+------+------------+\n",
            "| John| 28|  Mlae|    New York|\n",
            "| Jane| 32|Female|San Fransico|\n",
            "| Mike| 45|  Male| Los Angeles|\n",
            "|Emily| 23|Female|     Chicago|\n",
            "+-----+---+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEMPORARY AND GLOBAL VIEW**"
      ],
      "metadata": {
        "id": "M-QlCGJjtRze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE a temporary view\n",
        "df_people.createOrReplaceTempView(\"people_temp_view\")\n",
        "\n",
        "# Run a sql query on the VIEW\n",
        "result_temp_view = spark.sql(\"SELECT name,age,gender,city FROM people_temp_view WHERE age > 30\")\n",
        "\n",
        "#show the temp view result\n",
        "print(\"Temp view result:\")\n",
        "result_temp_view.show()\n",
        "\n",
        "# Create a global view\n",
        "df_people.createOrReplaceGlobalTempView(\"people_global_view\")\n",
        "\n",
        "\n",
        "# sql query for gloabl temp view\n",
        "result_global_view = spark.sql(\"SELECT name,age FROM global_temp.people_global_view WHERE age > 25\")\n",
        "\n",
        "# show the global view result\n",
        "print(\"Global view result:\")\n",
        "result_global_view.show()\n",
        "\n",
        "# List all temporary views and tables\n",
        "print(\"Listing all temporary views and tables:\")\n",
        "spark.catalog.listTables()\n",
        "\n",
        "# Drop the local temporary view\n",
        "print(\"Dropping temp view:\")\n",
        "spark.catalog.dropTempView(\"people_temp_view\")\n",
        "\n",
        "# Drop the global temporary view\n",
        "print(\"Dropping gloabl temp view:\")\n",
        "spark.catalog.dropGlobalTempView(\"people_global_view\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBzU8NytYRi",
        "outputId": "1f40ef68-d825-4239-9c72-4f800b499cbe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp view result:\n",
            "+----+---+------+------------+\n",
            "|name|age|gender|        city|\n",
            "+----+---+------+------------+\n",
            "|Jane| 32|Female|San Fransico|\n",
            "|Mike| 45|  Male| Los Angeles|\n",
            "+----+---+------+------------+\n",
            "\n",
            "Global view result:\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 28|\n",
            "|Jane| 32|\n",
            "|Mike| 45|\n",
            "+----+---+\n",
            "\n",
            "Listing all temporary views and tables:\n",
            "Dropping temp view:\n",
            "Dropping gloabl temp view:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}