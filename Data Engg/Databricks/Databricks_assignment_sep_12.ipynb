{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc03df8-33dc-4c52-8ae6-66704baefbee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**WORKING WITH CSV DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a638c46-119b-45ab-91ef-4450031d7d17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the CSV Data\n",
    "# Load the CSV data into a DataFrame\n",
    "df_employee = spark.read.csv(\"file:/Workspace/Users/azuser2115_mml.local@techademy.com/employee_data.txt\", header=True)\n",
    "\n",
    "# Display the first 10 rows and inspect the schema\n",
    "df_employee.show(10)\n",
    "df_employee.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83e405e-dfea-473f-ae9d-c1f54883952a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------+------+\n|EmployeeID|        Name|Department|JoiningDate|Salary|\n+----------+------------+----------+-----------+------+\n|      1001|    John Doe|        HR| 2021-01-15| 55000|\n|      1005|David Wilson|        IT| 2021-06-25| 58000|\n+----------+------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning\n",
    "# Remove rows where Salary is less than 55,000\n",
    "from pyspark.sql.functions import year, col\n",
    "df_cleaned_salary = df_employee.filter(col('Salary') >= 55000)\n",
    "\n",
    "# Filter employees who joined after the year 2020\n",
    "df_cleaned_joining = df_cleaned_salary.filter(year(col('JoiningDate')) > 2020)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned_joining.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf548fe-2d8f-421a-8cc2-ded23e68ffa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|Department|       avg(Salary)|\n+----------+------------------+\n|        HR|           54000.0|\n|   Finance|           68500.0|\n|        IT|61666.666666666664|\n+----------+------------------+\n\n+----------+-----+\n|Department|count|\n+----------+-----+\n|        HR|    3|\n|   Finance|    2|\n|        IT|    3|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#3. Data Aggregation\n",
    "# changing salary column into int\n",
    "df_employee = df_employee.withColumn(\"Salary\", col(\"Salary\").cast(\"int\"))\n",
    "\n",
    "# Find the average salary by Department\n",
    "df_avg_salary = df_employee.groupBy(\"Department\").avg(\"Salary\")\n",
    "df_avg_salary.show()\n",
    "\n",
    "# Count the number of employees in each Department\n",
    "df_employee_count = df_employee.groupBy(\"Department\").count()\n",
    "df_employee_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a210785-abf4-4365-9083-8a4f630ff58e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Write the Data to CSV\n",
    "# Save the cleaned data to a new CSV file\n",
    "df_cleaned_joining.write.csv(\"/Workspace/Users/azuser2115_mml.local@techademy.com/employee_data.txt\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1dd8486-e619-44a9-82a8-cfba58d70c93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "WORKING WITH JSON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d186580-6ff7-4d8f-ab4b-191d3cc57158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----+-----+\n|ProductID|ProductName|   Category|Price|Stock|\n+---------+-----------+-----------+-----+-----+\n|      101|     Laptop|Electronics| 1200|   35|\n|      102| Smartphone|Electronics|  800|   80|\n|      103| Desk Chair|  Furniture|  150|   60|\n|      104|    Monitor|Electronics|  300|   45|\n|      105|       Desk|  Furniture|  350|   25|\n+---------+-----------+-----------+-----+-----+\n\nroot\n |-- ProductID: integer (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Price: integer (nullable = true)\n |-- Stock: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = \"file:/Workspace/Users/azuser2115_mml.local@techademy.com/product_data.json\"\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Stock\", IntegerType(), True)\n",
    "])\n",
    "df_product = spark.read.schema(schema).json(data)\n",
    "\n",
    "# Display the first 10 rows and inspect the schema\n",
    "df_product.show(10)\n",
    "df_product.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc72aaa-e1a2-4c14-a3aa-480a28e56582",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----+-----+\n|ProductID|ProductName|   Category|Price|Stock|\n+---------+-----------+-----------+-----+-----+\n|      101|     Laptop|Electronics| 1200|   35|\n|      102| Smartphone|Electronics|  800|   80|\n|      104|    Monitor|Electronics|  300|   45|\n+---------+-----------+-----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning\n",
    "# Remove rows where Stock is less than 30\n",
    "df_cleaned_stock = df_product.filter(col('Stock') >= 30)\n",
    "\n",
    "# Filter the products that belong to the \"Electronics\" category\n",
    "df_electronics = df_cleaned_stock.filter(col('Category') == \"Electronics\")\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_electronics.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8519f40a-d2ac-4604-853c-26547e749642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|sum(Stock)|\n+----------+\n|        85|\n+----------+\n\n+----------+\n|avg(Price)|\n+----------+\n|     560.0|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Aggregation\n",
    "# Calculate the total stock for products in the \"Furniture\" category\n",
    "total_stock_furniture = df_product.filter(col('Category') == \"Furniture\").agg({\"Stock\": \"sum\"})\n",
    "total_stock_furniture.show()\n",
    "\n",
    "# Find the average price of all products\n",
    "avg_price = df_product.agg({\"Price\": \"avg\"})\n",
    "avg_price.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100e3ece-91e2-4f14-8319-0a1ac151e780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Write the Data to JSON\n",
    "# Save the cleaned and aggregated data to a new JSON file\n",
    "df_electronics.write.json(\"file:/Workspace/Users/azuser2115_mml.local@techademy.com/data.txt\",mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a77107-2d4c-4074-90bc-20888170cc4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**WORKING WITH DELTA TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1221ce-8900-4e7f-9d13-7ab80a843de8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert CSV and JSON Data to Delta Format\n",
    "# Convert the employee_data CSV to Delta format\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Users/azuser2115_mml.local@techademy.com/delta_employee_data\")\n",
    "\n",
    "# Convert the product_data JSON to Delta format\n",
    "df_product.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Users/azuser2115_mml.local@techademy.com/delta_product_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d00a44-521e-4946-ad00-835971caf5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Register Delta Tables\n",
    "\n",
    "delta_employee = spark.read.format(\"delta\").load(\"/Workspace/Users/azuser2115_mml.local@techademy.com/delta_employee_data\")\n",
    "delta_product = spark.read.format(\"delta\").load(\"/Workspace/Users/azuser2115_mml.local@techademy.com/delta_product_data\")\n",
    "\n",
    "# register delta tables as sql tables\n",
    "delta_employee.write.saveAsTable(\"employee_delta_table\")\n",
    "delta_product.write.saveAsTable(\"product_delta_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff01c63-48cd-4c9f-b4e3-e9cf78d34eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n+---------+-----------+-----------+-----+-----+\n|ProductID|ProductName|   Category|Price|Stock|\n+---------+-----------+-----------+-----+-----+\n|      101|     Laptop|Electronics| 1200|   35|\n|      102| Smartphone|Electronics|  800|   80|\n|      103| Desk Chair|  Furniture|  150|   60|\n|      104|    Monitor|Electronics|  300|   45|\n|      105|       Desk|  Furniture|  350|   25|\n+---------+-----------+-----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "delta_employee.show()\n",
    "delta_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d3a8c5-1369-4f10-93a3-942dd5c687f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 2020-03-10| 68355|\n|      1005| David Wilson|        IT| 2021-06-25| 63945|\n|      1007| James Miller|        IT| 2019-08-14| 71662|\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n+---------+-----------+-----------+-----+-----+\n|ProductID|ProductName|   Category|Price|Stock|\n+---------+-----------+-----------+-----+-----+\n|      102| Smartphone|Electronics|  800|   80|\n|      103| Desk Chair|  Furniture|  150|   60|\n|      104|    Monitor|Electronics|  300|   45|\n+---------+-----------+-----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Modifications with Delta Tables\n",
    "# Update operation: Increase the salary by 5% for all employees in the IT department\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE employee_delta_table\n",
    "    SET Salary = Salary * 1.05\n",
    "    WHERE Department = 'IT'\n",
    "\"\"\")\n",
    "\n",
    "# Delete operation: Delete products where the stock is less than 40\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM product_delta_table\n",
    "    WHERE Stock < 40\n",
    "\"\"\")\n",
    "spark.sql(\"select * from employee_delta_table\").show()\n",
    "spark.sql(\"select * from product_delta_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef44561f-6885-4f14-b405-d07cce4ea3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n+----------+-------------+----------+-----------+------+\n\n+---------+-----------+-----------+-----+-----+\n|ProductID|ProductName|   Category|Price|Stock|\n+---------+-----------+-----------+-----+-----+\n|      102| Smartphone|Electronics|  800|   80|\n+---------+-----------+-----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Query the employee Delta table to find employees in the Finance department\n",
    "df_finance_employees = spark.sql(\"SELECT * FROM employee_delta_table WHERE Department = 'Finance'\")\n",
    "df_finance_employees.show()\n",
    "\n",
    "# Query the product Delta table to find all products in the Electronics category with a price greater than 500\n",
    "df_electronics_expensive = spark.sql(\"SELECT * FROM product_delta_table WHERE Category = 'Electronics' AND Price > 500\")\n",
    "df_electronics_expensive.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-09-12 12:02:53",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
