{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3b0186b6-6f7b-4f60-a99f-0bcd730c5ef3","showTitle":false,"title":""},"id":"C8n6yWeRDzpd"},"source":["# DELTA LAKE CONCEPTS"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"d26bfc0f-4fe2-482e-9a4a-3d3f30b09a75","showTitle":true,"title":"Load the given CSV and JSON"},"id":"RHt6TghmDzph","outputId":"39796846-06dd-4c82-91b9-7acbf63343e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------+-----------+-----------+------+\n","|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n","+----------+------------+-----------+-----------+------+\n","|       101|        John|         HR| 2023-01-10| 50000|\n","|       102|       Alice|    Finance| 2023-02-15| 70000|\n","|       103|        Mark|Engineering| 2023-03-20| 85000|\n","|       104|        Emma|      Sales| 2023-04-01| 55000|\n","|       105|        Liam|  Marketing| 2023-05-12| 60000|\n","+----------+------------+-----------+-----------+------+\n","\n","+---------+-----------+-----------+------+\n","|ProductID|ProductName|   Category| Price|\n","+---------+-----------+-----------+------+\n","|     P101|     Laptop|Electronics|1200.0|\n","|     P102|      Phone|Electronics| 800.0|\n","|     P103|     Tablet|Electronics| 600.0|\n","|     P104|    Monitor|Electronics| 300.0|\n","|     P105|      Mouse|Accessories|  25.0|\n","+---------+-----------+-----------+------+\n","\n"]}],"source":["# copying files to dbfs\n","employee_csv_path = 'file:/Workspace/Shared/assignment17sep/employees.csv'\n","\n","new_employee_csv_path = 'file:/Workspace/Shared/assignment17sep/NewEmployeeData.csv'\n","\n","products_path = 'file:/Workspace/Shared/assignment17sep/products.json'\n","\n","dbutils.fs.cp(employee_csv_path, 'dbfs:/FileStore/assignment17sep/employees.csv')\n","\n","dbutils.fs.cp(new_employee_csv_path, 'dbfs:/FileStore/assignment17sep/NewEmployeeData.csv')\n","\n","dbutils.fs.cp(products_path, 'dbfs:/FileStore/assignment17sep/products.json')\n","\n","location = 'dbfs:/FileStore/assignment17sep/'\n","\n","# loading csv and json\n","employees_df = spark.read.csv(f\"{location}employees.csv\", header=True, inferSchema=True)\n","\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n","\n","schema = StructType([\n","    StructField(\"ProductID\", StringType(), True),\n","    StructField(\"ProductName\", StringType(), True),\n","    StructField(\"Category\", StringType(), True),\n","    StructField(\"Price\", DoubleType(), True)\n","])\n","\n","products_df = spark.read.schema(schema).json(f\"{location}products.json\")\n","\n","employees_df.show()\n","products_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"32151680-6065-4e0a-9812-55ac06317dd7","showTitle":true,"title":"converting into delta"},"id":"kPAEduyDDzpl"},"outputs":[],"source":["employees_df.write.format(\"delta\").save(f\"{location}delta/employees\")\n","products_df.write.format(\"delta\").save(f\"{location}delta/products\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"462e1615-709e-4eaf-984e-7c9ac11d591b","showTitle":true,"title":"Merge and Upsert (SCD)"},"id":"5vhXnAKeDzpm","outputId":"71f46989-96cc-4939-9c75-0b5ec2fded0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["New data appended to Delta table successfully.\n","Merging new data into Delta table...\n","Data merged successfully.\n"]}],"source":["new_employee_df = spark.read.csv(f\"{location}NewEmployeeData.csv\", header=True, inferSchema=True)\n","new_employee_df.write.format(\"delta\").mode(\"append\").save(f\"{location}delta/employees\")\n","print(\"New data appended to Delta table successfully.\")\n","\n","# Create a temporary view for SQL operations\n","new_employee_df.createOrReplaceTempView(\"new_employee_data\")\n","\n","print(\"Merging new data into Delta table...\")\n","\n","delta_table_path = 'dbfs:/FileStore/assignment17sep/delta/employees'\n","spark.sql(f\"\"\"\n","MERGE INTO delta.`{delta_table_path}` AS target\n","USING new_employee_data AS source\n","ON target.EmployeeID = source.EmployeeID\n","WHEN MATCHED THEN UPDATE SET\n","    target.Salary = source.Salary\n","WHEN NOT MATCHED THEN INSERT (EmployeeID,EmployeeName,Department,JoiningDate,Salary)\n","    VALUES (source.EmployeeID,source.EmployeeName,source.Department,source.JoiningDate,source.Salary)\n","\"\"\")\n","\n","print(\"Data merged successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"4747e0c5-7b3a-4f4a-afba-cddda36b735e","showTitle":true,"title":"Internals of Delta Table"},"id":"42IhW6tXDzpm","outputId":"798342d1-2783-4a63-cbca-8578c46dfac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Viewing Delta table history...\n","+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n","|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                                |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                                |\n","+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n","|5      |2024-09-17 07:26:46|8379095214579684|azuser2109_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                     |NULL|{1811550041510977}|0911-102451-o8x6anfh|4          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 2745, p25FileSize -> 1562, numDeletionVectorsRemoved -> 1, minFileSize -> 1562, numAddedFiles -> 1, maxFileSize -> 1562, p75FileSize -> 1562, p50FileSize -> 1562, numAddedBytes -> 1562}                                                                                                                                                                                                                                                                                                                                                                                                                                                     |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","|4      |2024-09-17 07:26:43|8379095214579684|azuser2109_mml.local@techademy.com|MERGE    |{predicate -> [\"(EmployeeID#2248 = EmployeeID#1914)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1811550041510977}|0911-102451-o8x6anfh|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1372, numTargetBytesRemoved -> 3777, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 7, executionTimeMs -> 5205, materializeSourceTimeMs -> 207, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 3012, numTargetRowsUpdated -> 7, numOutputRows -> 7, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 3, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1914}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","|3      |2024-09-17 07:26:35|8379095214579684|azuser2109_mml.local@techademy.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                          |NULL|{1811550041510977}|0911-102451-o8x6anfh|2          |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 2, numOutputBytes -> 1259}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","|2      |2024-09-17 07:24:51|8379095214579684|azuser2109_mml.local@techademy.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                          |NULL|{1811550041510977}|0911-102451-o8x6anfh|1          |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 2, numOutputBytes -> 1259}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","|1      |2024-09-17 07:24:27|8379095214579684|azuser2109_mml.local@techademy.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                          |NULL|{1811550041510977}|0911-102451-o8x6anfh|0          |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 2, numOutputBytes -> 1259}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","|0      |2024-09-17 07:24:16|8379095214579684|azuser2109_mml.local@techademy.com|WRITE    |{mode -> ErrorIfExists, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{1811550041510977}|0911-102451-o8x6anfh|NULL       |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1373}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n","+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n","\n","table before the previous merge operation.\n","+----------+------------+-----------+-----------+------+\n","|EmployeeID|EmployeeName|Department |JoiningDate|Salary|\n","+----------+------------+-----------+-----------+------+\n","|101       |John        |HR         |2023-01-10 |50000 |\n","|102       |Alice       |Finance    |2023-02-15 |70000 |\n","|103       |Mark        |Engineering|2023-03-20 |85000 |\n","|104       |Emma        |Sales      |2023-04-01 |55000 |\n","|105       |Liam        |Marketing  |2023-05-12 |60000 |\n","+----------+------------+-----------+-----------+------+\n","\n","Vacuuming old files...\n","Delta Table operations completed.\n"]}],"source":["# History\n","print(\"Viewing Delta table history...\")\n","history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n","history_df.show(truncate=False)\n","\n","# time travel\n","print(\"table before the previous merge operation.\")\n","path = 'dbfs:/FileStore/assignment17sep/delta/employees'\n","df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path)\n","df_time_travel.show(truncate=False)\n","\n","# Vacuum\n","print(\"Vacuuming old files...\")\n","spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS\")\n","\n","print(\"Delta Table operations completed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f9a520e7-3cc9-4fa5-a075-19624969647f","showTitle":true,"title":"Optimize Delta Table"},"id":"hF0NrcPpDzpn","outputId":"81a4ca9e-e641-4adb-ce81-e876d3f2c54c"},"outputs":[{"data":{"text/plain":["DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# delta table creation\n","spark.sql(\"CREATE TABLE IF NOT EXISTS delta_employee_table USING DELTA LOCATION 'dbfs:/FileStore/assignment17sep/delta/employees'\")\n","\n","# optimize\n","spark.sql(\"OPTIMIZE delta_employee_table\")\n","\n","# Zordering\n","spark.sql(\"OPTIMIZE delta_employee_table ZORDER BY Department\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2f404553-5d7b-4d14-9c71-2f822db12815","showTitle":true,"title":"Time Travel with Delta Table"},"id":"r-Ift39eDzpo","outputId":"6aa2f111-8a8c-458c-b8a6-698f45bbb6a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n","|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n","+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n","|     11|2024-09-13 04:17:28|8379095214579684|azuser2109_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3714719545832430}|0911-102451-o8x6anfh|         10|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n","|     10|2024-09-13 04:17:24|8379095214579684|azuser2109_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          9|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n","|      9|2024-09-13 04:17:02|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          8|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","|      8|2024-09-12 12:05:38|8379095214579684|azuser2109_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          7|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n","|      7|2024-09-12 12:05:36|8379095214579684|azuser2109_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          6|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n","|      6|2024-09-12 12:04:29|8379095214579684|azuser2109_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n","|      5|2024-09-12 12:04:25|8379095214579684|azuser2109_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          4|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n","|      4|2024-09-12 12:03:43|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          3|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","|      3|2024-09-12 12:03:30|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          2|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","|      2|2024-09-12 12:02:04|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          1|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","|      1|2024-09-12 12:01:24|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|          0|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","|      0|2024-09-12 12:01:04|8379095214579684|azuser2109_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3714719545832430}|0911-102451-o8x6anfh|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n","+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n","\n","+----------+-------------+----------+-----------+------+\n","|EmployeeID|         Name|Department|JoiningDate|Salary|\n","+----------+-------------+----------+-----------+------+\n","|      1001|     John Doe|        HR| 2021-01-15| 55000|\n","|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n","|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n","|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n","|      1005| David Wilson|        IT| 2021-06-25| 58000|\n","|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n","|      1007| James Miller|        IT| 2019-08-14| 65000|\n","|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n","+----------+-------------+----------+-----------+------+\n","\n"]}],"source":["# Check the history to find the version numbers\n","spark.sql(\"DESCRIBE HISTORY delta_employee_table;\").show()\n","\n","# Retrieve the Delta table as it was at a specific version\n","spark.sql(\"SELECT * FROM delta_employee_table VERSION AS OF 0;\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"54e043db-9421-40f1-a62c-afb0b220b2fb","showTitle":true,"title":"Vacuum Delta Table"},"id":"Saft71yMDzpo","outputId":"6a5088e8-6838-4fe9-f629-a0feea3210bc"},"outputs":[{"data":{"text/plain":["DataFrame[path: string]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# retain last 7 days record only\n","spark.sql(\"VACUUM delta_employee_table RETAIN 168 HOURS\")"]},{"cell_type":"markdown","metadata":{},"source":["# STRUCTURED STREAMING AND TRANSFORMATION ON STREAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"metadata":{},"output_type":"display_data"}],"source":["# folder for streaming\n","csv_path = 'file:/Workspace/Shared/assignment17sep/transactions.csv'\n","streaming_path ='dbfs:/FileStore/assignment17sep/streaming/input/'\n","dbutils.fs.cp(csv_path, f\"{streaming_path}transactions.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- TransactionID: string (nullable = true)\n"," |-- TransactionDate: date (nullable = true)\n"," |-- ProductID: string (nullable = true)\n"," |-- Quantity: integer (nullable = true)\n"," |-- Price: double (nullable = true)\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","spark = SparkSession.builder.appName(\"Streaming\").getOrCreate()\n","\n","transaction_schema = \"TransactionID STRING, TransactionDate DATE, ProductID STRING, Quantity INT, Price DOUBLE\"\n","transaction_stream_df = spark.readStream.format(\"csv\").option(\"header\", True).schema(transaction_schema) \\\n","            .load(\"dbfs:/FileStore/assignment17sep/streaming/input/\")\n","\n","transaction_stream_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+---------------+---------+--------+-----+-----------+\n","|TransactionID|TransactionDate|ProductID|Quantity|Price|TotalAmount|\n","+-------------+---------------+---------+--------+-----+-----------+\n","+-------------+---------------+---------+--------+-----+-----------+\n","\n"]}],"source":["transformed_stream_df = transaction_stream_df.withColumn(\"TotalAmount\", transaction_stream_df[\"Quantity\"] * transaction_stream_df[\"Price\"]).filter(transaction_stream_df[\"Quantity\"] > 1)\n","\n","query = transformed_stream_df.writeStream \\\n","    .format(\"memory\") \\\n","    .queryName(\"transformed_data\") \\\n","    .outputMode(\"append\") \\\n","    .start()\n","\n","# To view the data from memory\n","spark.sql(\"SELECT * FROM transformed_data\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import sum, col, to_timestamp\n","\n","transaction_stream_df = transaction_stream_df.withColumn(\"TransactionDate\", to_timestamp(col(\"TransactionDate\")))\n","\n","aggregated_stream_df = transaction_stream_df \\\n","    .withWatermark(\"TransactionDate\", \"1 day\") \\\n","    .groupBy(\"ProductID\") \\\n","    .agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalSales\"))\n","\n","query = aggregated_stream_df.writeStream \\\n","    .format(\"console\") \\\n","    .outputMode(\"update\") \\\n","    .start()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query = aggregated_stream_df.writeStream \\\n","    .format(\"parquet\") \\\n","    .option(\"path\", \"dbfs:/FileStore/assignment17sep/streaming/output/parquet/\") \\\n","    .option(\"checkpointLocation\", \"dbfs:/FileStore/assignment17sep/streaming/output/checkpoint/\") \\\n","    .outputMode(\"append\") \\\n","    .start()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["csv_path = 'file:/Workspace/Shared/assignment17sep/products.csv'\n","dbutils.fs.cp(csv_path, f\"{streaming_path}products.csv\")\n","\n","product_schema = \"ProductID STRING, ProductName STRING, Category STRING\"\n","product_stream_df = spark.readStream.format(\"csv\").option(\"header\", True).schema(product_schema) \\\n","    .load(\"dbfs:/FileStore/assignment17sep/streaming/input/\")\n","\n","joined_stream = transaction_stream_df.join(product_stream_df, \"ProductID\")\n","\n","query = joined_stream.writeStream \\\n","    .format(\"console\") \\\n","    .outputMode(\"append\") \\\n","    .start()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query.stop()\n","\n","query = aggregated_stream_df.writeStream \\\n","    .format(\"parquet\") \\\n","    .option(\"path\", \"dbfs:/FileStore/assignment17sep/streaming/output/parquet/\") \\\n","    .option(\"checkpointLocation\", \"dbfs:/FileStore/assignment17sep/streaming/output/checkpoint/\") \\\n","    .outputMode(\"append\") \\\n","    .start()"]},{"cell_type":"markdown","metadata":{},"source":["# CREATING A COMPLETE ETL PIPELINE USING DELTA LIVE TABLES"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"metadata":{},"output_type":"display_data"}],"source":["orders_csv_path = 'file:/Workspace/Shared/assignment17sep/orders.csv'\n","dbutils.fs.cp(orders_csv_path, \"dbfs:/FileStore/assignment17sep/orders.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["\n","<html>\n","  <style>\n","<style>\n","      html {\n","        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n","        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n","        Noto Color Emoji,FontAwesome;\n","        font-size: 13;\n","      }\n","\n","      .ansiout {\n","        padding-bottom: 8px;\n","      }\n","\n","      .createPipeline {\n","        background-color: rgb(34, 114, 180);\n","        color: white;\n","        text-decoration: none;\n","        padding: 4px 12px;\n","        border-radius: 4px;\n","        display: inline-block;\n","      }\n","\n","      .createPipeline:hover {\n","        background-color: #195487;\n","      }\n","\n","      .tag {\n","        border: none;\n","        color: rgb(31, 39, 45);\n","        padding: 2px 4px;\n","        font-weight: 600;\n","        background-color: rgba(93, 114, 131, 0.08);\n","        border-radius: 4px;\n","        margin-right: 0;\n","        display: inline-block;\n","        cursor: default;\n","      }\n","\n","      table {\n","        border-collapse: collapse;\n","        font-size: 13px;\n","      }\n","\n","      th {\n","        text-align: left;\n","        background-color: #F2F5F7;\n","        padding-left: 8px;\n","        padding-right: 8px;\n","      }\n","\n","      tr {\n","        border-bottom: solid;\n","        border-bottom-color: #CDDAE5;\n","        border-bottom-width: 1px;\n","      }\n","\n","      td {\n","        padding-left: 8px;\n","        padding-right: 8px;\n","      }\n","\n","      .dlt-label {\n","        font-weight: bold;\n","      }\n","\n","      ul {\n","        list-style: circle;\n","        padding-inline-start: 12px;\n","      }\n","\n","      li {\n","        padding-bottom: 4px;\n","      }\n","</style></style>\n","  \n","<div class=\"ansiout\">\n","<span class='tag'>orders_final</span> is defined as a\n","<span class=\"dlt-label\">Delta Live Tables</span> dataset\n"," with schema: \n","</div>\n","\n","  \n","<div class=\"ansiout\">\n","   <table>\n","     <tbody>\n","       <tr>\n","         <th>Name</th>\n","         <th>Type</th>\n","       </tr>\n","       \n","<tr>\n","   <td>OrderID</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>OrderDate</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>CustomerID</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>Product</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>Quantity</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>Price</td>\n","   <td>string</td>\n","</tr>\n","\n","<tr>\n","   <td>TotalAmount</td>\n","   <td>double</td>\n","</tr>\n","     </tbody>\n","   </table>\n","</div>\n","\n","  <div class =\"ansiout\">\n","    To populate your table you must either:\n","    <ul>\n","      <li>\n","        Run an existing pipeline using the\n","        <span class=\"dlt-label\">Delta Live Tables</span> menu\n","      </li>\n","      <li>\n","        Create a new pipeline: <a class='createPipeline' href=\"?o=412616323098688#joblist/pipelines/create?initialSource=%2FShared%2Fassignment17sep%2FDataBricks%20Assignment%20Sep17&redirectNotebookId=1811550041510977\">Create Pipeline</a>\n","      </li>\n","    </ul>\n","  <div>\n","</html>\n"]},"metadata":{},"output_type":"display_data"}],"source":["import dlt\n","from pyspark.sql.functions import col, expr\n","\n","# Read data from a CSV source\n","@dlt.table\n","def orders_raw():\n","    return spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/FileStore/assignment17sep/orders.csv\")\n","\n","# Transform data (add TotalAmount and filter Quantity > 1)\n","@dlt.table\n","def orders_transformed():\n","    df = dlt.read(\"orders_raw\")\n","    df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n","    return df.filter(col(\"Quantity\") > 1)\n","\n","# Load the transformed data into a Delta table\n","@dlt.table\n","def orders_final():\n","    dlt.read(\"orders_transformed\").write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/assignment17sep/delta/orders_final\")\n","    return dlt.read(\"orders_transformed\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+----------+-------+--------+-----+\n","|    _c0|       _c1|       _c2|    _c3|     _c4|  _c5|\n","+-------+----------+----------+-------+--------+-----+\n","|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n","|    101|2024-01-01|      C001| Laptop|       2| 1000|\n","|    102|2024-01-02|      C002|  Phone|       1|  500|\n","|    103|2024-01-03|      C003| Tablet|       3|  300|\n","|    104|2024-01-04|      C004|Monitor|       1|  150|\n","|    105|2024-01-05|      C005|  Mouse|       5|   20|\n","+-------+----------+----------+-------+--------+-----+\n","\n","+-------+----------+----------+--------+--------+-----+\n","|    _c0|       _c1|       _c2|     _c3|     _c4|  _c5|\n","+-------+----------+----------+--------+--------+-----+\n","|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n","|    101|2024-01-01|      C001|  Laptop|       2| 1000|\n","|    102|2024-01-02|      C002|   Phone|       1|  500|\n","|    103|2024-01-03|      C003|  Tablet|       3|  300|\n","|    104|2024-01-04|      C004| Monitor|       1|  150|\n","|    105|2024-01-05|      C005|   Mouse|       5|   20|\n","|    106|2024-01-12|      C006|Keyboard|       3|   50|\n","+-------+----------+----------+--------+--------+-----+\n","\n","+---+----------+----+------+---+----+------+\n","|_c0|       _c1| _c2|   _c3|_c4| _c5| Price|\n","+---+----------+----+------+---+----+------+\n","|101|2024-01-01|C001|Laptop|  2|1000|1100.0|\n","+---+----------+----+------+---+----+------+\n","\n","+---+----------+----+------+---+----+------+\n","|_c0|       _c1| _c2|   _c3|_c4| _c5| Price|\n","+---+----------+----+------+---+----+------+\n","|101|2024-01-01|C001|Laptop|  2|1000|1100.0|\n","+---+----------+----+------+---+----+------+\n","\n"]}],"source":["# python\n","df = spark.read.format(\"csv\").load(\"dbfs:/FileStore/assignment17sep/orders.csv\")\n","df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/assignment17sep/delta/orders\")\n","\n","# Read data from Delta Table\n","df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/assignment17sep/delta/orders\")\n","df.show()\n","\n","# Insert new record\n","df = df.union(spark.createDataFrame([(106, \"2024-01-12\", \"C006\", \"Keyboard\", 3, 50)], [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]))\n","df.show()\n","\n","# Update prices (increase price by 10%)\n","df = df.filter(col(\"_c3\") == \"Laptop\").withColumn(\"Price\", col(\"_c5\") * 1.1)\n","df.show()\n","\n","# Delete rows where Quantity < 2\n","df = df.filter(col(\"_c4\") >= 2)\n","df.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------+\n","|num_affected_rows|\n","+-----------------+\n","|                0|\n","+-----------------+\n","\n","+-----------------+\n","|num_affected_rows|\n","+-----------------+\n","|                0|\n","+-----------------+\n","\n"]},{"data":{"text/plain":["DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"]},"metadata":{},"output_type":"display_data"}],"source":["#SQL\n","\n","# Read data as Delta Table\n","spark.sql(\"CREATE TABLE IF NOT EXISTS delta_orders_table USING DELTA LOCATION 'dbfs:/FileStore/assignment17sep/delta/orders_final'\")\n","\n","# Update prices (increase laptops by 10%)\n","spark.sql(\"UPDATE delta_orders_table SET Price = Price * 1.1 WHERE Product = 'Laptop'\").show()\n","\n","# Delete rows where Quantity < 2\n","spark.sql(\"DELETE FROM delta_orders_table WHERE Quantity < 2\").show()\n","\n","# Insert new record\n","spark.sql(\"INSERT INTO delta_orders_table (OrderID, OrderDate, CustomerID, Product, Quantity, Price) VALUES (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Merging new data into Delta table...\n","New data merged successfully!\n"]}],"source":["data = [\n","    (101, '2024-01-10', 'C001', 'Laptop', 2, 1200), \n","    (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\n","    ]\n","    \n","schema = [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n","\n","\n","new_orders_df = spark.createDataFrame(data, schema=schema)\n","\n","new_orders_df.createOrReplaceTempView(\"new_orders_data\")\n","\n","print(\"Merging new data into Delta table...\")\n","\n","orders_df = spark.read.csv(\"dbfs:/FileStore/assignment17sep/orders.csv\", header=True, inferSchema=True)\n","orders_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/assignment17sep/delta/orders1\")\n","\n","\n","dbfs_path = 'dbfs:/FileStore/assignment17sep/delta/orders1'\n","spark.sql(f\"\"\"\n","MERGE INTO delta.`{dbfs_path}` AS target\n","USING new_orders_data AS source\n","ON target.OrderID = source.OrderID\n","WHEN MATCHED THEN UPDATE SET\n","    target.Quantity = source.Quantity, target.Price = source.Price\n","WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price) \n","VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n","\"\"\")\n","\n","print(\"New data merged successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>e81d9a0a-eb4b-4ed5-9e98-227a4ab6321a</td><td>null</td><td>null</td><td>dbfs:/FileStore/assignment17sep/delta/orders</td><td>2024-09-17T08:11:32.971Z</td><td>2024-09-17T08:27:18Z</td><td>List()</td><td>List()</td><td>6</td><td>9354</td><td>Map(delta.enableDeletionVectors -> true)</td><td>3</td><td>7</td><td>List(deletionVectors)</td><td>Map(numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0)</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql\n","-- View the history of changes\n","DESCRIBE HISTORY delta.`dbfs:/FileStore/assignment17sep/delta/orders`;\n","\n","-- View the detailed metadata\n","DESCRIBE DETAIL delta.`dbfs:/FileStore/assignment17sep/delta/orders`;\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>OrderID</td><td>OrderDate</td><td>CustomerID</td><td>Product</td><td>Quantity</td><td>Price</td></tr><tr><td>101</td><td>2024-01-01</td><td>C001</td><td>Laptop</td><td>2</td><td>1000</td></tr><tr><td>102</td><td>2024-01-02</td><td>C002</td><td>Phone</td><td>1</td><td>500</td></tr><tr><td>103</td><td>2024-01-03</td><td>C003</td><td>Tablet</td><td>3</td><td>300</td></tr><tr><td>104</td><td>2024-01-04</td><td>C004</td><td>Monitor</td><td>1</td><td>150</td></tr><tr><td>105</td><td>2024-01-05</td><td>C005</td><td>Mouse</td><td>5</td><td>20</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql\n","-- Query the table before the last merge\n","SELECT * FROM delta.`dbfs:/FileStore/assignment17sep/delta/orders` VERSION AS OF 0;\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[path: string]"]},"metadata":{},"output_type":"display_data"}],"source":["# Optimize the table for faster queries with Z-ordering\n","spark.sql(\"OPTIMIZE delta.`dbfs:/FileStore/assignment17sep/delta/orders`\")\n","\n","# Vacuum the table to remove old files\n","spark.sql(\"VACUUM delta.`dbfs:/FileStore/assignment17sep/delta/orders` RETAIN 168 HOURS\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# CREATING AND SCHEDULING A JOB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["csv_path = 'file:/Workspace/Shared/assignment17sep/orders.csv'\n","\n","dbfs_path = 'dbfs:/Filestore/assignment17sep/orders.csv'\n","\n","dbutils.fs.cp(csv_path, dbfs_path)\n","\n","from pyspark.sql.functions import col\n","\n","# Load CSV data\n","df = spark.read.format(\"csv\").option(\"header\", True).load(dbfs_path)\n","\n","# Add TotalAmount column and filter records (I used greater than 2 according to data)\n","transformed_df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")).filter(col(\"Quantity\") > 2)\n","\n","# Write to Delta table\n","transformed_df.write.format(\"delta\").mode(\"overwrite\")\\\n","    .save(\"dbfs:/Workspace/Shared/assignment17sep/orders_transformed\")\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"environmentMetadata":{"base_environment":"","client":"1"},"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"DataBricks Assignment Sep17","widgets":{}},"colab":{"provenance":[]},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
